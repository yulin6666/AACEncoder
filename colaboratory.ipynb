{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“欢迎使用 Colaboratory”的副本",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yulin6666/AACEncoder/blob/master/colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "UtIV2JJ1WQ4y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#IMDB 数据集\n",
        "from keras.datasets import imdb\n",
        "from keras import losses \n",
        "from keras import metrics\n",
        "from keras import models\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#训练集\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data( num_words=10000)\n",
        "\n",
        "print(test_labels[0])\n",
        "#数据预处理\n",
        "def vectorize_sequences(sequences, dimension=10000): \n",
        "  results = np.zeros((len(sequences), dimension)) \n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1\n",
        "  return results\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n",
        "#定义模型\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,))) \n",
        "model.add(layers.Dense(16, activation='relu')) \n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "#编译模型\n",
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
        "#训练模型\n",
        "history = model.fit(x_test, y_test,epochs=20,batch_size=512, validation_data=(x_train, y_train))\n",
        "#训练损失和验证损失\n",
        "history_dict = history.history \n",
        "loss_values = history_dict['acc'] \n",
        "val_loss_values = history_dict['val_acc']\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training acc') \n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation acc') \n",
        "plt.title('Training and validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('acc') \n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#预测\n",
        "print(model.predict(x_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0x48akuqKQD6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#路透社数据集\n",
        "from keras.datasets import reuters\n",
        "import numpy as np\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras import models \n",
        "from keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "#训练集\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data( num_words=10000)\n",
        "print(test_labels.shape)\n",
        "#预处理\n",
        "def vectorize_sequences(sequences, dimension=10000): \n",
        "  results = np.zeros((len(sequences), dimension)) \n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "y_train = to_categorical(train_labels)\n",
        "y_test = to_categorical(test_labels)\n",
        "print(y_test[0])\n",
        "#定义模型\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,))) \n",
        "model.add(layers.Dense(64, activation='relu')) \n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "#编译模型\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "#训练模型\n",
        "history = model.fit(x_test, y_test,epochs=20,batch_size=512, validation_data=(x_train, y_train))\n",
        "#训练损失和验证损失\n",
        "loss = history.history['loss'] \n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss') \n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss') \n",
        "plt.title('Training and validation loss') \n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss') \n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.clf() #清空图像\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc') \n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc') \n",
        "plt.title('Training and validation accuracy') \n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy') \n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#预测模型\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "print(np.argmax(predictions[0]))\n",
        "print(y_test[0])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}